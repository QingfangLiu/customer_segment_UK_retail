{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QingfangLiu/DS_learning/blob/main/Challenge_Employee_Attrition_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKU2obxkKH8c"
      },
      "source": [
        "# Welcome the challenge notebook\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvBVXrZHlWX8"
      },
      "source": [
        "In this challenge, you will work with a dataset provided by an HR manager who wants to predict which employees are at risk of leaving the company. The dataset contains four key performance indicators (KPIs) related to each employee. Your task is to use PySpark to build a machine learning model that can predict employee attrition and to identify which KPI is most strongly associated with attrition in this company.\n",
        "\n",
        "- Please note that the dataset is already clean and ready to be modeled.\n",
        "- The dataset only contains numerical features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1xXctOSKf36"
      },
      "source": [
        "Installing pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLd6XoJtJZP4"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gA6GqvzKpR6"
      },
      "source": [
        "Importing the needed modules and creating the spark session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbkE-lH6Jmbd",
        "outputId": "318754f7-decc-4cd2-a590-eb1ba39022f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (9.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly) (24.2)\n"
          ]
        }
      ],
      "source": [
        "pip install plotly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-7M-8cGJmbd"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "c-YsrJbjKptK",
        "outputId": "4a113674-dbc9-4857-9065-cbe9db078278"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7aba5b77a150>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://dcab849e1f29:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Challenge</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# importing spark session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# data visualization modules\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "\n",
        "# pandas module\n",
        "import pandas as pd\n",
        "\n",
        "# pyspark data preprocessing modules\n",
        "from pyspark.ml.feature import  VectorAssembler, StandardScaler,StringIndexer\n",
        "\n",
        "# pyspark data modeling and model evaluation modules\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# creating the spark session\n",
        "spark = SparkSession.builder.appName(\"Challenge\").getOrCreate()\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn6fCpN5K5jb"
      },
      "source": [
        "Loading the `Challenge_dataset.csv` file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EKyTAZYKyTr"
      },
      "outputs": [],
      "source": [
        "data = spark.read.format('csv').option('header',True).option('inferSchema',True).load('Challenge_dataset.csv')\n",
        "data.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5CQl_LEVHW3"
      },
      "source": [
        "Create the numerical feature vector using `Vector Assembler`.\n",
        "\n",
        "Hint: The numerical input features are the KPIs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLhqklGYVDsx"
      },
      "outputs": [],
      "source": [
        "# write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2JgbGQtqOh2"
      },
      "source": [
        "Apply `Standard Scaler` to the numerical feature vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0DVLkyrWTF_"
      },
      "outputs": [],
      "source": [
        "# write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPWtvnpsq0z6"
      },
      "source": [
        "Split the data into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7MhYhqtq1rg"
      },
      "outputs": [],
      "source": [
        "# write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMOG53waYjoI"
      },
      "source": [
        "Train your Decision Tree model. Use `maxDepth = 3`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUhsVtYQa5bm"
      },
      "outputs": [],
      "source": [
        "# write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKbBtzIlrT1x"
      },
      "source": [
        "Perform the prediction on the test set and calculate the accuracy using `BinaryClassificationEvaluator`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Leo_RWmprjCj"
      },
      "outputs": [],
      "source": [
        "# write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWwHgTp9sBp_"
      },
      "source": [
        "Apply the hyper paramter tuning to find the proper `maxDepth` for your decision tree from the `candidates` list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p23wO0LAr0FG"
      },
      "outputs": [],
      "source": [
        "def evaluate_dt(mode_params):\n",
        "      test_accuracies = []\n",
        "      train_accuracies = []\n",
        "\n",
        "      for maxD in mode_params:\n",
        "        # train the model based on the maxD\n",
        "        decision_tree = DecisionTreeClassifier(featuresCol = 'input_features', labelCol = 'CurrentEmployee', maxDepth = maxD)\n",
        "        dtModel = decision_tree.fit(train)\n",
        "\n",
        "        # calculating test error\n",
        "        predictions_test = dtModel.transform(test)\n",
        "        evaluator = BinaryClassificationEvaluator(labelCol='CurrentEmployee')\n",
        "        auc_test = evaluator.evaluate(predictions_test, {evaluator.metricName: \"areaUnderROC\"})\n",
        "        # recording the accuracy\n",
        "        test_accuracies.append(auc_test)\n",
        "\n",
        "        # calculating training error\n",
        "        predictions_training = dtModel.transform(train)\n",
        "        evaluator = BinaryClassificationEvaluator(labelCol='CurrentEmployee')\n",
        "        auc_training = evaluator.evaluate(predictions_training, {evaluator.metricName: \"areaUnderROC\"})\n",
        "        train_accuracies.append(auc_training)\n",
        "\n",
        "      return(test_accuracies, train_accuracies)\n",
        "\n",
        "\n",
        "\n",
        "candidates = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
        "\n",
        "# write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8AduBzoteop"
      },
      "source": [
        "Use a line chart to visualize the training and testing accuracy. <br>\n",
        "\n",
        "Hint: To visualize your data, convert the PySpark dataframe to pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISSL9xj7tKpK"
      },
      "outputs": [],
      "source": [
        "# write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLqPW95IwuE5"
      },
      "source": [
        "Train the decision tree using the proper `maxDepth` parameter.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y39G2g9xtdwo"
      },
      "outputs": [],
      "source": [
        "# write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hN329Mg-xFc-"
      },
      "source": [
        "Use the `Feature Importance` to find the most important factor for the employee attrition using a barchart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDpAGaKJxEsK"
      },
      "outputs": [],
      "source": [
        "# write your code here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}